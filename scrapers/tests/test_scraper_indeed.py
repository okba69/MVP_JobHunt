"""
ğŸ” Job Hunter OS â€” Test Scraper Indeed (Playwright)
===================================================
Script de test pour scraper des offres d'emploi sur Indeed.fr
Utilise Playwright en mode headless pour naviguer comme un vrai navigateur.

Usage:
    python3 test_scraper_indeed.py
    python3 test_scraper_indeed.py --query "dÃ©veloppeur python" --location "Paris"
    python3 test_scraper_indeed.py --query "data analyst" --location "Lyon" --pages 3
"""

import argparse
import csv
import json
import random
import time
from datetime import datetime
from dataclasses import dataclass, asdict

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout


# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DEFAULT_QUERY = "dÃ©veloppeur"
DEFAULT_LOCATION = "Paris"
DEFAULT_PAGES = 1
BASE_URL = "https://fr.indeed.com"

# DÃ©lais alÃ©atoires pour simuler un humain (en secondes)
MIN_DELAY = 2
MAX_DELAY = 5


# â”€â”€â”€ ModÃ¨le de donnÃ©es â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class JobOffer:
    """ReprÃ©sente une offre d'emploi extraite d'Indeed."""
    titre: str
    entreprise: str
    lieu: str
    url: str
    description_courte: str
    date_scraping: str
    source: str = "indeed"


# â”€â”€â”€ Fonctions utilitaires â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def human_delay():
    """Attend un dÃ©lai alÃ©atoire pour imiter un humain."""
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    print(f"  â³ Pause de {delay:.1f}s...")
    time.sleep(delay)


def build_search_url(query: str, location: str, start: int = 0) -> str:
    """Construit l'URL de recherche Indeed."""
    from urllib.parse import quote_plus
    url = f"{BASE_URL}/jobs?q={quote_plus(query)}&l={quote_plus(location)}"
    if start > 0:
        url += f"&start={start}"
    return url


# â”€â”€â”€ Scraper principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scrape_indeed(query: str, location: str, max_pages: int = 1) -> list[JobOffer]:
    """
    Scrape les offres d'emploi depuis Indeed.fr avec Playwright.

    Args:
        query: Mots-clÃ©s de recherche (ex: "dÃ©veloppeur python")
        location: Ville ou rÃ©gion (ex: "Paris")
        max_pages: Nombre de pages de rÃ©sultats Ã  scraper

    Returns:
        Liste d'objets JobOffer
    """
    all_offers: list[JobOffer] = []

    print(f"\n{'='*60}")
    print(f"ğŸ” Recherche Indeed : '{query}' Ã  '{location}'")
    print(f"   Pages Ã  scraper : {max_pages}")
    print(f"{'='*60}\n")

    with sync_playwright() as p:
        # Lancer le navigateur en mode headless (invisible)
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
            ]
        )

        # CrÃ©er un contexte avec un User-Agent rÃ©aliste
        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1920, "height": 1080},
            locale="fr-FR",
        )

        page = context.new_page()

        # Bloquer les ressources inutiles pour aller plus vite
        page.route("**/*.{png,jpg,jpeg,gif,svg,ico,woff,woff2}", lambda route: route.abort())
        page.route("**/analytics**", lambda route: route.abort())
        page.route("**/tracking**", lambda route: route.abort())

        for page_num in range(max_pages):
            start = page_num * 10
            url = build_search_url(query, location, start)

            print(f"ğŸ“„ Page {page_num + 1}/{max_pages} â€” {url}")

            try:
                # Naviguer vers la page de rÃ©sultats
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
                human_delay()

                # GÃ©rer le popup de cookies s'il apparaÃ®t
                try:
                    cookie_btn = page.locator(
                        "button#onetrust-accept-btn-handler, "
                        "button[aria-label='Accepter'], "
                        "button:has-text('Tout accepter'), "
                        "button:has-text('Accepter')"
                    )
                    if cookie_btn.count() > 0:
                        cookie_btn.first.click(timeout=3000)
                        print("  ğŸª Popup cookies fermÃ©")
                        time.sleep(1)
                except Exception:
                    pass  # Pas de popup, on continue

                # Chercher les cartes d'offres d'emploi
                # Indeed utilise plusieurs sÃ©lecteurs possibles selon la version du site
                job_cards = page.locator(
                    "div.job_seen_beacon, "
                    "div.jobsearch-ResultsList > div, "
                    "li.css-5lfssm, "
                    "div[data-jk], "
                    "td.resultContent"
                )

                count = job_cards.count()
                print(f"  ğŸ“‹ {count} offres trouvÃ©es sur cette page")

                if count == 0:
                    # Sauvegarder le HTML pour debug
                    debug_html = page.content()
                    debug_path = f"debug_indeed_page_{page_num + 1}.html"
                    with open(debug_path, "w", encoding="utf-8") as f:
                        f.write(debug_html)
                    print(f"  âš ï¸  Aucune offre trouvÃ©e ! HTML sauvegardÃ© dans {debug_path}")
                    print(f"  ğŸ’¡ Le site a peut-Ãªtre dÃ©tectÃ© le scraping ou la structure a changÃ©.")

                    # VÃ©rifier si on est bloquÃ©
                    if "captcha" in debug_html.lower() or "blocked" in debug_html.lower():
                        print("  ğŸš« CAPTCHA ou blocage dÃ©tectÃ© ! ArrÃªt du scraping.")
                        break
                    continue

                # Extraire les donnÃ©es de chaque offre
                for i in range(count):
                    try:
                        card = job_cards.nth(i)

                        # Titre du poste
                        title_el = card.locator(
                            "h2.jobTitle a, "
                            "a[data-jk], "
                            "span[id^='jobTitle'], "
                            "h2 a"
                        )
                        titre = title_el.first.inner_text(timeout=2000).strip() if title_el.count() > 0 else "N/A"

                        # Lien de l'offre
                        link_el = card.locator("h2 a, a[data-jk], a.jcs-JobTitle")
                        href = ""
                        if link_el.count() > 0:
                            href = link_el.first.get_attribute("href") or ""
                            if href.startswith("/"):
                                href = BASE_URL + href

                        # Entreprise
                        company_el = card.locator(
                            "span[data-testid='company-name'], "
                            "span.css-1h7lukg, "
                            "span.companyName, "
                            "a[data-tn-element='companyName']"
                        )
                        entreprise = company_el.first.inner_text(timeout=2000).strip() if company_el.count() > 0 else "N/A"

                        # Lieu
                        location_el = card.locator(
                            "div[data-testid='text-location'], "
                            "div.css-1restlb, "
                            "div.companyLocation"
                        )
                        lieu = location_el.first.inner_text(timeout=2000).strip() if location_el.count() > 0 else "N/A"

                        # Description courte
                        desc_el = card.locator(
                            "div.css-9446fg, "
                            "div.job-snippet, "
                            "ul[style] li, "
                            "table.jobCardShelfContainer"
                        )
                        description = ""
                        if desc_el.count() > 0:
                            description = desc_el.first.inner_text(timeout=2000).strip()[:200]

                        offer = JobOffer(
                            titre=titre,
                            entreprise=entreprise,
                            lieu=lieu,
                            url=href,
                            description_courte=description,
                            date_scraping=datetime.now().isoformat(),
                        )
                        all_offers.append(offer)
                        print(f"  âœ… {i+1}. {titre} â€” {entreprise} ({lieu})")

                    except Exception as e:
                        print(f"  âš ï¸  Erreur sur l'offre {i+1}: {e}")
                        continue

            except PlaywrightTimeout:
                print(f"  â° Timeout sur la page {page_num + 1}, on passe Ã  la suivante")
                continue
            except Exception as e:
                print(f"  âŒ Erreur inattendue : {e}")
                break

            # Pause entre les pages
            if page_num < max_pages - 1:
                human_delay()

        browser.close()

    # DÃ©doublonner par URL (Indeed affiche parfois le mÃªme job dans 2 containers)
    seen_urls: set[str] = set()
    unique_offers: list[JobOffer] = []
    for offer in all_offers:
        key = offer.url or f"{offer.titre}|{offer.entreprise}"
        if key not in seen_urls:
            seen_urls.add(key)
            unique_offers.append(offer)

    if len(unique_offers) < len(all_offers):
        print(f"\nğŸ”„ DÃ©doublonnage : {len(all_offers)} â†’ {len(unique_offers)} offres uniques")

    return unique_offers


# â”€â”€â”€ Export des rÃ©sultats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_to_csv(offers: list[JobOffer], filename: str = "resultats_indeed.csv"):
    """Sauvegarde les offres dans un fichier CSV."""
    if not offers:
        print("\nâš ï¸  Aucune offre Ã  sauvegarder.")
        return

    with open(filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=list(asdict(offers[0]).keys()))
        writer.writeheader()
        for offer in offers:
            writer.writerow(asdict(offer))

    print(f"\nğŸ’¾ {len(offers)} offres sauvegardÃ©es dans '{filename}'")


def save_to_json(offers: list[JobOffer], filename: str = "resultats_indeed.json"):
    """Sauvegarde les offres dans un fichier JSON."""
    if not offers:
        return

    data = [asdict(o) for o in offers]
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ {len(offers)} offres sauvegardÃ©es dans '{filename}'")


def print_summary(offers: list[JobOffer]):
    """Affiche un rÃ©sumÃ© des offres trouvÃ©es."""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š RÃ‰SUMÃ‰ â€” {len(offers)} offres extraites")
    print(f"{'='*60}")

    if not offers:
        print("  Aucune offre trouvÃ©e.")
        return

    # Stats par entreprise
    companies: dict[str, int] = {}
    for o in offers:
        companies[o.entreprise] = companies.get(o.entreprise, 0) + 1

    print(f"\n  ğŸ¢ Entreprises uniques : {len(companies)}")
    print(f"  ğŸ“ Lieux uniques : {len(set(o.lieu for o in offers))}")

    print(f"\n  Top entreprises :")
    for company, count in sorted(companies.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"    â€¢ {company} ({count} offre{'s' if count > 1 else ''})")

    # AperÃ§u des premiÃ¨res offres
    print(f"\n  ğŸ“‹ AperÃ§u (5 premiÃ¨res) :")
    for i, o in enumerate(offers[:5], 1):
        print(f"    {i}. {o.titre}")
        print(f"       {o.entreprise} â€” {o.lieu}")
        if o.description_courte:
            print(f"       {o.description_courte[:80]}...")
        print()


# â”€â”€â”€ Point d'entrÃ©e â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(
        description="ğŸ” Scraper Indeed.fr â€” Test Job Hunter OS",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation :
  python3 test_scraper_indeed.py
  python3 test_scraper_indeed.py --query "data analyst" --location "Lyon"
  python3 test_scraper_indeed.py --query "chef de projet" --location "Marseille" --pages 3
        """
    )
    parser.add_argument("--query", "-q", default=DEFAULT_QUERY,
                        help=f"Mots-clÃ©s de recherche (dÃ©faut: '{DEFAULT_QUERY}')")
    parser.add_argument("--location", "-l", default=DEFAULT_LOCATION,
                        help=f"Ville ou rÃ©gion (dÃ©faut: '{DEFAULT_LOCATION}')")
    parser.add_argument("--pages", "-p", type=int, default=DEFAULT_PAGES,
                        help=f"Nombre de pages Ã  scraper (dÃ©faut: {DEFAULT_PAGES})")

    args = parser.parse_args()

    # Lancer le scraping
    offers = scrape_indeed(
        query=args.query,
        location=args.location,
        max_pages=args.pages,
    )

    # Afficher le rÃ©sumÃ©
    print_summary(offers)

    # Sauvegarder les rÃ©sultats
    if offers:
        save_to_csv(offers)
        save_to_json(offers)
        print("\nâœ… Test terminÃ© avec succÃ¨s !")
    else:
        print("\nâš ï¸  Aucune offre rÃ©cupÃ©rÃ©e.")
        print("   Causes possibles :")
        print("   â€¢ Indeed a dÃ©tectÃ© le scraping (CAPTCHA)")
        print("   â€¢ La structure HTML a changÃ©")
        print("   â€¢ ProblÃ¨me de connexion rÃ©seau")
        print("   ğŸ’¡ VÃ©rifie le fichier debug_indeed_page_1.html pour diagnostiquer")


if __name__ == "__main__":
    main()
