"""
ğŸ” Job Hunter OS â€” Test Scraper LinkedIn Jobs (Playwright)
==========================================================
Script de test pour scraper des offres d'emploi sur LinkedIn.
Utilise la page publique LinkedIn Jobs (pas besoin de login).
Playwright en mode headless pour naviguer comme un vrai navigateur.

Usage:
    python3 test_scraper_linkedin.py
    python3 test_scraper_linkedin.py --query "data analyst" --location "Lyon"
    python3 test_scraper_linkedin.py --query "dÃ©veloppeur python" --location "Paris" --pages 3
"""

import argparse
import csv
import json
import random
import time
from datetime import datetime
from dataclasses import dataclass, asdict

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout


# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DEFAULT_QUERY = "dÃ©veloppeur"
DEFAULT_LOCATION = "Paris"
DEFAULT_PAGES = 1
RESULTS_PER_PAGE = 25  # LinkedIn affiche 25 offres par page

# DÃ©lais alÃ©atoires pour simuler un humain (en secondes)
MIN_DELAY = 3
MAX_DELAY = 6


# â”€â”€â”€ ModÃ¨le de donnÃ©es â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class JobOffer:
    """ReprÃ©sente une offre d'emploi extraite de LinkedIn."""
    titre: str
    entreprise: str
    lieu: str
    url: str
    date_publication: str
    description_courte: str
    date_scraping: str
    source: str = "linkedin"


# â”€â”€â”€ Fonctions utilitaires â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def human_delay(min_s: float = MIN_DELAY, max_s: float = MAX_DELAY):
    """Attend un dÃ©lai alÃ©atoire pour imiter un humain."""
    delay = random.uniform(min_s, max_s)
    print(f"  â³ Pause de {delay:.1f}s...")
    time.sleep(delay)


def build_search_url(query: str, location: str, start: int = 0) -> str:
    """Construit l'URL de recherche LinkedIn Jobs (page publique)."""
    from urllib.parse import quote_plus
    url = f"https://www.linkedin.com/jobs/search/?keywords={quote_plus(query)}&location={quote_plus(location)}"
    if start > 0:
        url += f"&start={start}"
    return url


# â”€â”€â”€ Scraper principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scrape_linkedin(query: str, location: str, max_pages: int = 1) -> list[JobOffer]:
    """
    Scrape les offres d'emploi depuis LinkedIn Jobs avec Playwright.
    Utilise la page publique (pas besoin de login).

    Args:
        query: Mots-clÃ©s de recherche (ex: "data analyst")
        location: Ville ou rÃ©gion (ex: "Paris")
        max_pages: Nombre de pages de rÃ©sultats Ã  scraper

    Returns:
        Liste d'objets JobOffer
    """
    all_offers: list[JobOffer] = []

    print(f"\n{'='*60}")
    print(f"ğŸ” Recherche LinkedIn : '{query}' Ã  '{location}'")
    print(f"   Pages Ã  scraper : {max_pages}")
    print(f"{'='*60}\n")

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-dev-shm-usage",
            ]
        )

        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1920, "height": 1080},
            locale="fr-FR",
            extra_http_headers={
                "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            }
        )

        page = context.new_page()

        # Bloquer les ressources inutiles pour aller plus vite
        page.route("**/*.{png,jpg,jpeg,gif,svg,ico,woff,woff2}", lambda route: route.abort())
        page.route("**/li/track**", lambda route: route.abort())
        page.route("**/analytics**", lambda route: route.abort())

        for page_num in range(max_pages):
            start = page_num * RESULTS_PER_PAGE
            url = build_search_url(query, location, start)

            print(f"ğŸ“„ Page {page_num + 1}/{max_pages} â€” {url}")

            try:
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
                human_delay()

                # GÃ©rer le popup cookies LinkedIn
                try:
                    cookie_btn = page.locator(
                        "button[action-type='ACCEPT'], "
                        "button:has-text('Accepter'), "
                        "button:has-text('Accept'), "
                        "button.artdeco-global-alert__action"
                    )
                    if cookie_btn.count() > 0:
                        cookie_btn.first.click(timeout=3000)
                        print("  ğŸª Popup cookies fermÃ©")
                        time.sleep(1)
                except Exception:
                    pass

                # Fermer le popup "Rejoignez LinkedIn" s'il apparaÃ®t
                try:
                    dismiss_btn = page.locator(
                        "button[data-tracking-control-name='public_jobs_contextual-sign-in-modal_modal_dismiss'], "
                        "button.modal__dismiss, "
                        "icon[data-test-icon='close-medium']"
                    ).first
                    if dismiss_btn.is_visible(timeout=2000):
                        dismiss_btn.click()
                        print("  âŒ Popup login fermÃ©")
                        time.sleep(1)
                except Exception:
                    pass

                # Scroller vers le bas pour charger toutes les offres (lazy loading)
                print("  ğŸ“œ Scroll pour charger les offres...")
                for scroll_i in range(5):
                    page.evaluate("window.scrollBy(0, 800)")
                    time.sleep(random.uniform(0.5, 1.2))

                # Remonter en haut
                page.evaluate("window.scrollTo(0, 0)")
                time.sleep(1)

                # Chercher les cartes d'offres (LinkedIn public job search)
                job_cards = page.locator(
                    "div.base-card, "
                    "li.jobs-search-results__list-item, "
                    "div.job-search-card, "
                    "ul.jobs-search__results-list > li"
                )

                count = job_cards.count()
                print(f"  ğŸ“‹ {count} offres trouvÃ©es sur cette page")

                if count == 0:
                    # Sauvegarder le HTML pour debug
                    debug_html = page.content()
                    debug_path = f"debug_linkedin_page_{page_num + 1}.html"
                    with open(debug_path, "w", encoding="utf-8") as f:
                        f.write(debug_html)
                    print(f"  âš ï¸  Aucune offre trouvÃ©e ! HTML sauvegardÃ© dans {debug_path}")

                    if "authwall" in debug_html.lower() or "sign in" in debug_html.lower():
                        print("  ğŸ”’ LinkedIn demande un login. La page publique est peut-Ãªtre bloquÃ©e.")
                    if "captcha" in debug_html.lower():
                        print("  ğŸš« CAPTCHA dÃ©tectÃ© ! ArrÃªt du scraping.")
                        break
                    continue

                # Extraire les donnÃ©es de chaque offre
                for i in range(count):
                    try:
                        card = job_cards.nth(i)

                        # Titre du poste
                        title_el = card.locator(
                            "h3.base-search-card__title, "
                            "a.base-card__full-link, "
                            "h3.job-search-card__title, "
                            "span.sr-only"
                        )
                        titre = ""
                        if title_el.count() > 0:
                            titre = title_el.first.inner_text(timeout=2000).strip()

                        if not titre:
                            continue

                        # Lien de l'offre
                        link_el = card.locator(
                            "a.base-card__full-link, "
                            "a[data-tracking-control-name='public_jobs_jserp-result_search-card']"
                        )
                        href = ""
                        if link_el.count() > 0:
                            href = link_el.first.get_attribute("href") or ""
                            # Nettoyer l'URL (retirer les paramÃ¨tres de tracking)
                            if "?" in href:
                                href = href.split("?")[0]

                        # Entreprise
                        company_el = card.locator(
                            "h4.base-search-card__subtitle, "
                            "a.hidden-nested-link, "
                            "h4.job-search-card__company-name"
                        )
                        entreprise = company_el.first.inner_text(timeout=2000).strip() if company_el.count() > 0 else "N/A"

                        # Lieu
                        location_el = card.locator(
                            "span.job-search-card__location, "
                            "span.base-search-card__metadata"
                        )
                        lieu = location_el.first.inner_text(timeout=2000).strip() if location_el.count() > 0 else "N/A"

                        # Date de publication
                        date_el = card.locator(
                            "time, "
                            "span.job-search-card__listdate"
                        )
                        date_pub = ""
                        if date_el.count() > 0:
                            date_pub = date_el.first.get_attribute("datetime") or date_el.first.inner_text(timeout=2000).strip()

                        offer = JobOffer(
                            titre=titre,
                            entreprise=entreprise,
                            lieu=lieu,
                            url=href,
                            date_publication=date_pub,
                            description_courte="",  # NÃ©cessiterait de cliquer sur chaque offre
                            date_scraping=datetime.now().isoformat(),
                        )
                        all_offers.append(offer)
                        print(f"  âœ… {i+1}. {titre} â€” {entreprise} ({lieu})")

                    except Exception as e:
                        print(f"  âš ï¸  Erreur sur l'offre {i+1}: {e}")
                        continue

            except PlaywrightTimeout:
                print(f"  â° Timeout sur la page {page_num + 1}, on passe Ã  la suivante")
                continue
            except Exception as e:
                print(f"  âŒ Erreur inattendue : {e}")
                break

            # Pause entre les pages
            if page_num < max_pages - 1:
                human_delay(4, 7)

        browser.close()

    # DÃ©doublonner par URL
    seen_urls: set[str] = set()
    unique_offers: list[JobOffer] = []
    for offer in all_offers:
        key = offer.url or f"{offer.titre}|{offer.entreprise}"
        if key not in seen_urls:
            seen_urls.add(key)
            unique_offers.append(offer)

    if len(unique_offers) < len(all_offers):
        print(f"\nğŸ”„ DÃ©doublonnage : {len(all_offers)} â†’ {len(unique_offers)} offres uniques")

    return unique_offers


# â”€â”€â”€ Export des rÃ©sultats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_to_csv(offers: list[JobOffer], filename: str = "resultats_linkedin.csv"):
    """Sauvegarde les offres dans un fichier CSV."""
    if not offers:
        print("\nâš ï¸  Aucune offre Ã  sauvegarder.")
        return

    with open(filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=list(asdict(offers[0]).keys()))
        writer.writeheader()
        for offer in offers:
            writer.writerow(asdict(offer))

    print(f"\nğŸ’¾ {len(offers)} offres sauvegardÃ©es dans '{filename}'")


def save_to_json(offers: list[JobOffer], filename: str = "resultats_linkedin.json"):
    """Sauvegarde les offres dans un fichier JSON."""
    if not offers:
        return

    data = [asdict(o) for o in offers]
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ {len(offers)} offres sauvegardÃ©es dans '{filename}'")


def print_summary(offers: list[JobOffer]):
    """Affiche un rÃ©sumÃ© des offres trouvÃ©es."""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š RÃ‰SUMÃ‰ â€” {len(offers)} offres extraites")
    print(f"{'='*60}")

    if not offers:
        print("  Aucune offre trouvÃ©e.")
        return

    companies: dict[str, int] = {}
    for o in offers:
        companies[o.entreprise] = companies.get(o.entreprise, 0) + 1

    print(f"\n  ğŸ¢ Entreprises uniques : {len(companies)}")
    print(f"  ğŸ“ Lieux uniques : {len(set(o.lieu for o in offers))}")

    print(f"\n  Top entreprises :")
    for company, count in sorted(companies.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"    â€¢ {company} ({count} offre{'s' if count > 1 else ''})")

    print(f"\n  ğŸ“‹ AperÃ§u (5 premiÃ¨res) :")
    for i, o in enumerate(offers[:5], 1):
        print(f"    {i}. {o.titre}")
        print(f"       {o.entreprise} â€” {o.lieu}")
        if o.date_publication:
            print(f"       ğŸ“… {o.date_publication}")
        print()


# â”€â”€â”€ Point d'entrÃ©e â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(
        description="ğŸ” Scraper LinkedIn Jobs â€” Test Job Hunter OS",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation :
  python3 test_scraper_linkedin.py
  python3 test_scraper_linkedin.py --query "data analyst" --location "Lyon"
  python3 test_scraper_linkedin.py --query "chef de projet" --location "Marseille" --pages 2
        """
    )
    parser.add_argument("--query", "-q", default=DEFAULT_QUERY,
                        help=f"Mots-clÃ©s de recherche (dÃ©faut: '{DEFAULT_QUERY}')")
    parser.add_argument("--location", "-l", default=DEFAULT_LOCATION,
                        help=f"Ville ou rÃ©gion (dÃ©faut: '{DEFAULT_LOCATION}')")
    parser.add_argument("--pages", "-p", type=int, default=DEFAULT_PAGES,
                        help=f"Nombre de pages Ã  scraper (dÃ©faut: {DEFAULT_PAGES})")

    args = parser.parse_args()

    offers = scrape_linkedin(
        query=args.query,
        location=args.location,
        max_pages=args.pages,
    )

    print_summary(offers)

    if offers:
        save_to_csv(offers)
        save_to_json(offers)
        print("\nâœ… Test terminÃ© avec succÃ¨s !")
    else:
        print("\nâš ï¸  Aucune offre rÃ©cupÃ©rÃ©e.")
        print("   Causes possibles :")
        print("   â€¢ LinkedIn bloque le scraping (authwall/CAPTCHA)")
        print("   â€¢ La structure HTML a changÃ©")
        print("   â€¢ ProblÃ¨me de connexion rÃ©seau")
        print("   ğŸ’¡ VÃ©rifie le fichier debug_linkedin_page_1.html pour diagnostiquer")


if __name__ == "__main__":
    main()
